{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Deep Learning Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go into deep learning modelling, we will first need to have a quick familiarisation with a deep learning framework. We recommend __[Keras](https://keras.io)__, which is built on top of Tensorflow, but alternatively, you can consider __[PyTorch](https://pytorch.org)__. Resources are abundant online on how to use them, but here are some official guides to get you started:\n",
    "- PyTorch has a [60 Minute Blitz Guide](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html)\n",
    "- Tensorflow has an [Intro to Keras guide](https://www.tensorflow.org/guide/keras)\n",
    "\n",
    "A few words on the difference between Keras and PyTorch - Keras is a high level wrapper on top of Google's Tensorflow, the most popular deep learning framework out there. Being more low level, Tensorflow faces many issues and troubles, which are addressed by the abstractions of Keras, making it a great way to start. Facebook's PyTorch on the other hand is a newcomer which has received massive interest in recent years, and is playing catch up to Tensorflow/Keras.\n",
    "\n",
    "If you are more interested in how deep learning software has evolved since the days of Caffe and Theano as well as more in depth into what is happening in the software behind the scenes, we also recommend a [full lecture from Stanford](https://www.youtube.com/watch?v=6SlgtELqOWc) on this topic, although this is extra knowledge that isn't fully critical to this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the tutorials you go through, you should be ready to build a 2 (or more) layer Multi-Level Perceptron (MLP) with deep learning. With the dataset you have prepared your machine learning model in the previous section, run your data through a MLP model with `Dense` (`Linear`) layers instead. Do some slight model adjustments, and discuss what kind of adjustments lead to improvements in score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.week4_func as wk4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data transformations/preprocessing\n",
    "\n",
    "Most neural networks expect the images of a fixed size. Therefore, you will need to write some prepocessing code. At the basic level, you will need to normalise the data. Use the appropriate data generator/loader methods to encapsulate your data for training purposes. Do the same for both the train and test (and val, if exist) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# load and normalize data\n",
    "df_1 = wk4.just_dataframes('./data/cifar-10-batches-py/data_batch_1')\n",
    "df = pd.concat([df_1,wk4.just_dataframes('./data/cifar-10-batches-py/data_batch_2')],axis=0)\n",
    "df = pd.concat([df,wk4.just_dataframes('./data/cifar-10-batches-py/data_batch_3')],axis=0)\n",
    "df = pd.concat([df,wk4.just_dataframes('./data/cifar-10-batches-py/data_batch_4')],axis=0)\n",
    "df = pd.concat([df,wk4.just_dataframes('./data/cifar-10-batches-py/data_batch_5')],axis=0)\n",
    "df_test = wk4.just_dataframes('./data/cifar-10-batches-py/test_batch')\n",
    "X_all = df.drop('target',axis=1).values/255\n",
    "\n",
    "# X_train,X_val,y_train,y_val = train_test_split(X_all,df['target'],test_size=0.05,random_state=42,shuffle=True)\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X_train,y_train,test_size=0.21,random_state=42,shuffle=True)\n",
    "X_train = X_all\n",
    "y_train = df['target'].values\n",
    "X_test = df_test.drop('target',axis=1).values/255\n",
    "y_test = df_test['target'].values\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data object\n",
    "class cifar10(Dataset):\n",
    "    '''Inherited class from torch.utils.data.Dataset\n",
    "    this method is more useful if you are dynamically reading samples\n",
    "    from disk. Otherwise, just use the TensorDataset method.'''\n",
    "    def __init__(self, X_train, y_train, transform=None):\n",
    "        self.X = torch.from_numpy(X_train).float()\n",
    "        self.y = torch.from_numpy(y_train)\n",
    "        self.y2 = y_train\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.y2)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx,:]\n",
    "        target = self.y[idx]\n",
    "        sample = {'image': image, 'target': target}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "# cifar_data = cifar10(X_train,y_train)\n",
    "# trainloader = DataLoader(cifar_data, batch_size=10, shuffle=True)\n",
    "# testloader = DataLoader(cifar10(X_test,y_test), batch_size=10, shuffle=False)\n",
    "\n",
    "'''Found a faster way to load data'''\n",
    "train_data = TensorDataset(torch.from_numpy(X_train).float(),torch.from_numpy(y_train))\n",
    "trainloader = DataLoader(train_data,batch_size=10,shuffle=True)\n",
    "test_data = TensorDataset(torch.from_numpy(X_test).float(),torch.from_numpy(y_test))\n",
    "testloader = DataLoader(test_data,batch_size=10,shuffle=False)\n",
    "# for i in range(len(cifar_data)): # Just print this to make sure it worked. '0 torch.Size([3072]) torch.Size([])'\n",
    "#     sample = cifar_data[i]\n",
    "#     print(i, sample['image'].size(), sample['target'].size())\n",
    "#     if i == 3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Build multi-layer perceptron neural network models with Keras \n",
    "\n",
    "The Keras Python library for deep learning focuses on the creation of models as a sequence of layers.\n",
    "\n",
    "In here, you will discover the simple components that you can use to create neural networks and simple deep learning models using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play cheat using torchvision dataset\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "#                                         download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "#                                        download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat',\n",
    "#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 16 * 5 * 5)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "class FlatNet(nn.Module):\n",
    "    '''Build 2-layer NN'''\n",
    "    def __init__(self):\n",
    "        super(FlatNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "net = FlatNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4078, 0.3451, 0.4275,  ..., 0.1569, 0.2000, 0.0863],\n",
       "        [0.0784, 0.0824, 0.0784,  ..., 0.0941, 0.0902, 0.0902],\n",
       "        [0.1608, 0.2667, 0.2353,  ..., 0.3137, 0.3451, 0.3529],\n",
       "        ...,\n",
       "        [0.4980, 0.4627, 0.4549,  ..., 0.2588, 0.2745, 0.2471],\n",
       "        [0.3843, 0.4588, 0.4902,  ..., 0.3176, 0.3137, 0.3059],\n",
       "        [0.7647, 0.6353, 0.6196,  ..., 0.6314, 0.6196, 0.6824]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = next(iter(trainloader))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.969\n",
      "[1,  4000] loss: 1.804\n",
      "[2,  2000] loss: 1.693\n",
      "[2,  4000] loss: 1.671\n",
      "[3,  2000] loss: 1.602\n",
      "[3,  4000] loss: 1.597\n",
      "[4,  2000] loss: 1.559\n",
      "[4,  4000] loss: 1.546\n",
      "[5,  2000] loss: 1.521\n",
      "[5,  4000] loss: 1.499\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def train_model(net,trainloader,criterion,optimizier):\n",
    "    for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i,data in enumerate(trainloader):\n",
    "            # get the inputs\n",
    "#             inputs, labels = data['image'] , data['target'] # using the self-constructed data object\n",
    "            inputs, labels = data #otherwise\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    return net\n",
    "net = train_model(net,trainloader,criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 44 %\n"
     ]
    }
   ],
   "source": [
    "def scoring(net,testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "#             images, labels = data['image'],data['target'] # if using object data constructor\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "scoring(net,testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the MLP network in CIFAR-10\n",
    "\n",
    "The main objective is to train the MLP network to achieve a balance between the ability to respond correctly to the input patterns that are used for training and the ability to provide good response to the input that is similar. Use the stochastic gradient descent optimiser with an appropriate learning rate between 1e-2 and 1e-3. Report your evaluation loss and accuracy, and you can also consider doing things like early stopping to prevent overfitting and achieve the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.477\n",
      "[1,  4000] loss: 1.479\n",
      "[2,  2000] loss: 1.445\n",
      "[2,  4000] loss: 1.450\n",
      "[3,  2000] loss: 1.417\n",
      "[3,  4000] loss: 1.431\n",
      "[4,  2000] loss: 1.402\n",
      "[4,  4000] loss: 1.408\n",
      "[5,  2000] loss: 1.387\n",
      "[5,  4000] loss: 1.388\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 48 %\n",
      "[1,  2000] loss: 1.487\n",
      "[1,  4000] loss: 1.484\n",
      "[2,  2000] loss: 1.450\n",
      "[2,  4000] loss: 1.449\n",
      "[3,  2000] loss: 1.421\n",
      "[3,  4000] loss: 1.433\n",
      "[4,  2000] loss: 1.402\n",
      "[4,  4000] loss: 1.412\n",
      "[5,  2000] loss: 1.387\n",
      "[5,  4000] loss: 1.395\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 47 %\n",
      "[1,  2000] loss: 1.445\n",
      "[1,  4000] loss: 1.468\n",
      "[2,  2000] loss: 1.443\n",
      "[2,  4000] loss: 1.453\n",
      "[3,  2000] loss: 1.431\n",
      "[3,  4000] loss: 1.424\n",
      "[4,  2000] loss: 1.410\n",
      "[4,  4000] loss: 1.412\n",
      "[5,  2000] loss: 1.397\n",
      "[5,  4000] loss: 1.420\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 44 %\n",
      "[1,  2000] loss: 1.467\n",
      "[1,  4000] loss: 1.493\n",
      "[2,  2000] loss: 1.457\n",
      "[2,  4000] loss: 1.472\n",
      "[3,  2000] loss: 1.449\n",
      "[3,  4000] loss: 1.477\n",
      "[4,  2000] loss: 1.423\n",
      "[4,  4000] loss: 1.454\n",
      "[5,  2000] loss: 1.432\n",
      "[5,  4000] loss: 1.429\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 46 %\n",
      "[1,  2000] loss: 1.496\n",
      "[1,  4000] loss: 1.528\n",
      "[2,  2000] loss: 1.499\n",
      "[2,  4000] loss: 1.507\n",
      "[3,  2000] loss: 1.493\n",
      "[3,  4000] loss: 1.499\n",
      "[4,  2000] loss: 1.478\n",
      "[4,  4000] loss: 1.474\n",
      "[5,  2000] loss: 1.449\n",
      "[5,  4000] loss: 1.488\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 44 %\n",
      "[1,  2000] loss: 1.516\n",
      "[1,  4000] loss: 1.574\n",
      "[2,  2000] loss: 1.538\n",
      "[2,  4000] loss: 1.550\n",
      "[3,  2000] loss: 1.528\n",
      "[3,  4000] loss: 1.541\n",
      "[4,  2000] loss: 1.512\n",
      "[4,  4000] loss: 1.534\n",
      "[5,  2000] loss: 1.516\n",
      "[5,  4000] loss: 1.506\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 40 %\n",
      "[1,  2000] loss: 1.558\n",
      "[1,  4000] loss: 1.597\n",
      "[2,  2000] loss: 1.569\n",
      "[2,  4000] loss: 1.611\n",
      "[3,  2000] loss: 1.565\n",
      "[3,  4000] loss: 1.585\n",
      "[4,  2000] loss: 1.578\n",
      "[4,  4000] loss: 1.571\n",
      "[5,  2000] loss: 1.550\n",
      "[5,  4000] loss: 1.580\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 38 %\n",
      "[1,  2000] loss: 1.645\n",
      "[1,  4000] loss: 1.633\n",
      "[2,  2000] loss: 1.633\n",
      "[2,  4000] loss: 1.654\n",
      "[3,  2000] loss: 1.637\n",
      "[3,  4000] loss: 1.651\n",
      "[4,  2000] loss: 1.628\n",
      "[4,  4000] loss: 1.684\n",
      "[5,  2000] loss: 1.620\n",
      "[5,  4000] loss: 1.661\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 39 %\n",
      "[1,  2000] loss: 1.678\n",
      "[1,  4000] loss: 1.735\n",
      "[2,  2000] loss: 1.742\n",
      "[2,  4000] loss: 1.708\n",
      "[3,  2000] loss: 1.696\n",
      "[3,  4000] loss: 1.712\n",
      "[4,  2000] loss: 1.699\n",
      "[4,  4000] loss: 1.718\n",
      "[5,  2000] loss: 1.690\n",
      "[5,  4000] loss: 1.721\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 37 %\n",
      "[1,  2000] loss: 1.782\n",
      "[1,  4000] loss: 1.778\n",
      "[2,  2000] loss: 1.777\n",
      "[2,  4000] loss: 1.797\n",
      "[3,  2000] loss: 1.782\n",
      "[3,  4000] loss: 1.790\n",
      "[4,  2000] loss: 1.771\n",
      "[4,  4000] loss: 1.782\n",
      "[5,  2000] loss: 1.778\n",
      "[5,  4000] loss: 1.800\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 31 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for lr in np.linspace(1e-3,1e-2,10):\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "    net = train_model(net,trainloader,criterion,optimizer)\n",
    "    scoring(net,testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3env)",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
