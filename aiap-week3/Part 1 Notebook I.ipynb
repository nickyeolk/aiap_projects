{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's assignment consists of two parts. The first part gives you an introduction to unsupervised learning. In particular, we focus on techniques for clustering and dimensionality reduction and how they can be applied to ecommerce data. As you work through the three clustering case studies, you will find yourself generating many intermediate datasets, trying different models, and tuning each model as you go along. There's a lot to keep track of.   \n",
    "\n",
    "This is where Part 2 comes in. It is in Part 2 that we introduce ideas of **workflow management** and **computational reproducibility**. Workflow management means organising your project directory to manage your analysis' artefacts (visualisations, processed datasets, notebooks and utility functions and experiment results). Ideally, your code for these should be clearly commented with well chosen names. Computational reproducibility means someone else (including future you!) being able to take just the code and data, and reproduce your project, from its results and models to visualisations etc. How one decides to practice workflow management and computational reproducibility can be quite a personal decision. Therefore, we provide guidelines, not rules. The most important is having a system rather than no system at all. \n",
    "\n",
    "**note about the week**   \n",
    "While week 1's assignment was guided, with specific instructions about what code to run, as we move on the assignments will involve less hand-holding. For this week, we include some instructions, but leave the specific implementations up to you. There are also many techniques we cover. Again, while we share some resources, we leave the bulk of the research and background reading up to you to manage for yourself. As a record of how well you have synthesised your readings, this notebook has space for writing a few paragraphs of explanations. We hope you won't be content with a shallow explanation, and that you use this space to apply the [Feynmann technique](https://collegeinfogeek.com/feynman-technique/) as a check of understanding. \n",
    "\n",
    "To tie Part 1 and Part 2 of the week together, we are not just asking for one Jupyter Notebook as a final deliverable. Instead, there are four deliverables to the week: \n",
    "\n",
    "**1. Part 1 Notebook I and II exercises presented at the end of the week job-interview style**\n",
    "\n",
    "**2. A script that automates the data cleaning and feature engineering steps you take so your findings are reproducible (see Part 2 Notebook I for more info)**\n",
    "\n",
    "**3. A repo README.md (see Part 2 Notebook I for more info)**\n",
    "\n",
    "**4  A record of software package versions and software environment used (see Part 2 Notebook I for more info)**   \n",
    "\n",
    "**recap of the objectives for the first 6 weeks:**  \n",
    "We aim to broadly cover a wide range of Machine Learning algorithms so that you can: \n",
    "- handle the technical demands of a 100E given some guidance on the right direction to take \n",
    "- can handle a technical job interview and get hired \n",
    "\n",
    "*materials for unsupervised learning adapted from William Thji* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Unsupervised Learning \n",
    "Unsupervised Learning refers to a set of machine learning techniques where no output variables (Y) are given. Only the input variables (X) are available and our job is to find patterns in X. You may read more about it from *pg 485 from Hastie and Tibshirani's Elements of Statistical Learning* available [here](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). \n",
    "\n",
    "ESL by Hastie et. al with be the primary reference for this week, although feel free to source for your own books and links. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short introduction to clustering \n",
    "Clustering puts datapoints into subsets so that datapoints within a cluster are more closely related to one another compared to datapoints in another cluster. More information is available from page 501 of *Elements of Statistical Learning*. \n",
    "\n",
    "Some quick points: \n",
    "- Clustering is extremely useful to many fields: \n",
    "    - Customer segmentation for personalised product recommendations\n",
    "    - Topic identification to relieve the need to manually vet documents \n",
    "    - Image or geo-spatial segmentation to optimised supply and demand (Gojek does this) \n",
    "    - and maybe most importantly, getting a sense of the data before starting to model it. \n",
    "\n",
    "- Some examples of clustering algorithms: \n",
    "    - KMeans\n",
    "    - Gaussian Mixture Models for drawing soft clustering boundaries instead of hard ones \n",
    "    - Hierarchical clustering\n",
    "    - DBScan for density-based clustering for anomaly detection \n",
    "    - Co-clustering\n",
    "    - Biclustering for analysing genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 1: Complete the Clustering Case Study: Using PCA and clustering to uncover customer segments \n",
    "Context: The dataset we will be working with contains ecommerce transactions from a UK-based online retails store. The dataset is available on [Kaggle](https://www.kaggle.com/carrie1/ecommerce-data/home) or the UCI Machine Learning Repository. The dataset is quite small, so we have also included it inside the `data` folder inside this repo as `data/raw/data.csv`. \n",
    "\n",
    "From the Kaggle website: \n",
    "\n",
    "\"This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\"\n",
    "\n",
    "### Case Study Goal \n",
    "Cluster the dataset into meaningful customer segments. This means creating clusters that are statistically robust and that make business sense. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1    536365     71053                  WHITE METAL LANTERN         6   \n",
       "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "\n",
       "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
       "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
       "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df_raw = pd.read_csv('data/raw/data.csv', encoding='ISO-8859-1')\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data \n",
    "\n",
    "Some data types are muddled, there are duplicates, NA values and unreasonable values hiding in the columns \n",
    "\n",
    "1. Clean the dataset. You may want to list the steps taken to clean the data and encapsulate the steps inside their own functions so they can be reused. Organise the functions into their own library\n",
    "\n",
    "<font color='blue'>Deal with negative prices, negative quantities (which are returns maybe)  \n",
    "Invoice Number: Drop those that start with A, contra against those starting with C.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      "InvoiceNo      541909 non-null object\n",
      "StockCode      541909 non-null object\n",
      "Description    540455 non-null object\n",
      "Quantity       541909 non-null int64\n",
      "InvoiceDate    541909 non-null object\n",
      "UnitPrice      541909 non-null float64\n",
      "CustomerID     406829 non-null float64\n",
      "Country        541909 non-null object\n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 33.1+ MB\n",
      "None\n",
      "            Quantity      UnitPrice     CustomerID\n",
      "count  541909.000000  541909.000000  406829.000000\n",
      "mean        9.552250       4.611114   15287.690570\n",
      "std       218.081158      96.759853    1713.600303\n",
      "min    -80995.000000  -11062.060000   12346.000000\n",
      "25%         1.000000       1.250000   13953.000000\n",
      "50%         3.000000       2.080000   15152.000000\n",
      "75%        10.000000       4.130000   16791.000000\n",
      "max     80995.000000   38970.000000   18287.000000\n",
      "InvoiceNo           0\n",
      "StockCode           0\n",
      "Description      1454\n",
      "Quantity            0\n",
      "InvoiceDate         0\n",
      "UnitPrice           0\n",
      "CustomerID     135080\n",
      "Country             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_raw.info())\n",
    "print(df_raw.describe())\n",
    "print(df_raw.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial len 541909\n",
      "accounting entries 3\n",
      "inventory adjustment items removed: 1336\n",
      "Price at or below 0 removed:  1179\n",
      "Customers with no ID:  132602\n",
      "Quantities at or below 0 removed:  8905\n",
      "Duplicates 5192\n",
      "final len 392692\n"
     ]
    }
   ],
   "source": [
    "def remove_accounting(df):\n",
    "    print('accounting entries',len(df[df['InvoiceNo'].str.contains('^[A]')]))\n",
    "    return df[~df['InvoiceNo'].str.contains('^[A]')]\n",
    "\n",
    "def remove_inventory_management(df):\n",
    "    # Items with a negative quantity and InvoiceNo not starting with C are removed\n",
    "    print('inventory adjustment items removed:',len(df_raw[((df_raw['Quantity']<0) & (df_raw['InvoiceNo'].str.contains('^(?!C)')))]))\n",
    "    return df[~((df['Quantity']<0) & (df['InvoiceNo'].str.contains('^(?!C)')))]\n",
    "\n",
    "def remove_neg_price(df):\n",
    "    print('Price at or below 0 removed: ',len(df[df['UnitPrice']<=0]))\n",
    "    return df[df['UnitPrice']>0]\n",
    "\n",
    "def remove_ghost_cust(df):\n",
    "    print('Customers with no ID: ',np.sum(df['CustomerID'].isnull()))\n",
    "    df.dropna(axis=0,subset=['CustomerID'],inplace=True)\n",
    "    return df\n",
    "\n",
    "def remove_neg_quantities(df):\n",
    "    print('Quantities at or below 0 removed: ',len(df[df['Quantity']<=0]))\n",
    "    return df[df['Quantity']>0]\n",
    "\n",
    "def remove_dupe(df):\n",
    "    print('Duplicates',len(df[df.duplicated(keep='first')]))\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def clean_data(df): # This function calls all the clearning steps\n",
    "    print('initial len',len(df))\n",
    "    df = remove_accounting(df)\n",
    "    df = remove_inventory_management(df)\n",
    "    df = remove_neg_price(df)\n",
    "    df = remove_ghost_cust(df)\n",
    "    df = remove_neg_quantities(df)\n",
    "    df = remove_dupe(df)\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    print('final len',len(df))\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean[df_clean['Quantity']<0]\n",
    "# df_clean[(df_clean['StockCode']=='22556')&(abs(df_clean['CustomerID']-17548)<0.01)]\n",
    "# df_clean[df_clean['CustomerID']==12476]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum(df_raw['Quantity']<0))\n",
    "# print(sum(df_raw[df_raw['Quantity']<0]['InvoiceNo'].str.contains('^[C]')))\n",
    "# print(sum(df_raw[df_raw['Quantity']<0]['InvoiceNo'].str.contains('^(?!C)')))\n",
    "# df_raw[~((df_raw['Quantity']<0) & (df_raw['InvoiceNo'].str.contains('^(?!C)')))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering iteration #1 \n",
    "\n",
    "Inside the dataset, each row contains information about an ecommerce transation. However, we want to cluster the data by customers, which means each row should instead contain information about a customer. \n",
    "\n",
    "1. Reshape the data to follow the format below: \n",
    "![alt text](customer.png)\n",
    "\n",
    "The dataframe should have these columns: `['NoOfInvoices', 'NoOfUniqueItems', 'QuantityPerInvoice', 'TotalQuantity', 'UniqueItemsPerInvoice','UnitPriceMean','UnitPriceStd']` \n",
    "2. Save this dataset as an intermediate dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate',\n",
       "       'UnitPrice', 'CustomerID', 'Country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here \n",
    "def engineer_features1(df):\n",
    "    grouped = df.groupby('CustomerID')\n",
    "    NoOfInvoices = grouped['InvoiceNo'].nunique()\n",
    "    NoOfUniqueItems = grouped['StockCode'].nunique()\n",
    "    QuantityPerInvoice = grouped['Quantity'].sum()/NoOfInvoices\n",
    "    TotalQuantity = grouped['Quantity'].sum()\n",
    "    UniqueItemsPerInvoice = NoOfUniqueItems/NoOfInvoices\n",
    "    UnitPriceMean = grouped['UnitPrice'].mean()\n",
    "    UnitPriceStd = grouped['UnitPrice'].std()\n",
    "    UnitPriceStd.fillna(0,inplace=True)\n",
    "    df_new = pd.DataFrame({'NoOfInvoices':NoOfInvoices,'NoOfUniqueItems':NoOfUniqueItems,'QuantityPerInvoice':QuantityPerInvoice,\\\n",
    "                          'TotalQuantity':TotalQuantity,'UniqueItemsPerInvoice':UniqueItemsPerInvoice,\\\n",
    "                           'UnitPriceMean':UnitPriceMean,'UnitPriceStd':UnitPriceStd})\n",
    "    df_new.index = df_new.index.astype(int)\n",
    "    return df_new\n",
    "\n",
    "df_new = engineer_features1(df_clean)\n",
    "df_new.to_pickle('./data/engineered_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering iteration #1 [reference pg 520](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "\n",
    "1. Implement any more preprocessing or visualisation steps you feel are necessary to understand how you might build meaningful clusters from the data. \n",
    "2. Apply hierarchical clustering to the dataset. \n",
    "2. Experiment with different linkage algorithms. Visualise the resulting trees for average linkage, complete linkage and single linkage side-by-side. Which linkage algorithm works best? \n",
    "3. List two ways to improve the clustering and implement at least one. Track the results of the first iteration and second iteration (whether with Excel, TextEdit or within this notebook itself). \n",
    "4. Describe the clusters you selected and evaluate whether or not they form meaningful segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "%matplotlib inline\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "df_clus = pd.read_pickle('./data/engineered_df.pkl')\n",
    "\n",
    "\n",
    "features = ['QuantityPerInvoice','UniqueItemsPerInvoice']\n",
    "features = df_clus.columns\n",
    "labelList = df_clus.index\n",
    "X = df_clus[features]\n",
    "\n",
    "def make_dendogram(X):\n",
    "    linked = linkage(X, 'single')\n",
    "    plt.figure(figsize=(10, 7))  \n",
    "    dendrogram(linked,  \n",
    "                orientation='top',\n",
    "                labels=labelList,\n",
    "                distance_sort='descending',\n",
    "                show_leaf_counts=True)\n",
    "    plt.show()  \n",
    "    \n",
    "def make_clustering(X,nclusters):\n",
    "    cluster = AgglomerativeClustering(n_clusters=nclusters, affinity='euclidean', linkage='ward')  \n",
    "    cluster.fit_predict(X) \n",
    "    fig = plt.figure(figsize=(10, 7))  \n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X.iloc[:,0], X.iloc[:,1], X.iloc[:,3], c=cluster.labels_, cmap='rainbow',alpha=0.7) \n",
    "#     # rotate the axes and update\n",
    "#     for angle in range(0, 360):\n",
    "#         ax.view_init(30, angle)\n",
    "#         plt.draw()\n",
    "#         plt.pause(.001)\n",
    "    \n",
    "make_dendogram(X)\n",
    "make_clustering(X,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means and GMM Clustering iteration #1 [reference pg 509](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "Apart from hierarchical clustering, we can also apply KMeans and Gaussian Mixture Models (GMM) on the data\n",
    "\n",
    "1. Implement K-means clustering on the data, experimenting with different values of k \n",
    "2. Implement a Gaussian Mixture Model on the data, experimenting with different values for the number of components.  \n",
    "3. Visualise how well K-means and GMMs succeed at separating subgroups in the data\n",
    "4. List two ways to improve the clustering and implement at least one. \n",
    "5. Describe the clusters and evaluate whether or not they form meaningful segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers \n",
    "\n",
    "1. Do outliers influence the results of your Hierarchical Clustering, K-means and GMM models?  \n",
    "2. Do the outliers themselves form clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating clusters  \n",
    "\n",
    "1. Use at least three techniques to validate that your chosen clusters correspond to meaningful customer segments. You may consider a combination of using visualisations and/or quantitative metrics (refer to Elements of Statistical Learning for some examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "1. Explain when it is appropriate to use K-Means, GMM and Hierarchical Clustering \n",
    "2. Which model would you choose for this dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening the black box of clustering [reference pg 503](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "\n",
    "1. In your own words, explain the broad idea behind building a proximity matrix to cluster data. (This should also be verbally presented during Monday's presentation). \n",
    "*Sometimes, typing values into MS Excel helps with building intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In your own words, how would you calculate the disimilarity between objects in a dataset? (This should also be verbally presented during Monday's presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Dimensionality Reduction \n",
    "\n",
    "1. Using the dataset that you have pre-processed, construct a pairplot again. How is it different from the a plot of the raw data and what is your interpretation?  \n",
    "2. Apply PCA pre-processed dataset. More information on PCA here [link]\n",
    "3. Create a plot of cumulative explained variance and number of components. How does this inform you about the best number of components to select? \n",
    "4. Create a plot of PC0 against PC1, coloured by the GMM's predictions on the normalised dataset with outliers removed for n_components =7\n",
    "5. How would you connect the Princip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
